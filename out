============================= test session starts ==============================
platform linux -- Python 3.12.11, pytest-8.4.1, pluggy-1.6.0 -- /home/bahuang/.conda/envs/pytorch-3.12/bin/python3.12
cachedir: .pytest_cache
hypothesis profile 'default' -> database=DirectoryBasedExampleDatabase('/data/users/bahuang/pytorch/.hypothesis/examples')
rootdir: /data/users/bahuang/pytorch
configfile: pytest.ini
plugins: xdoctest-1.1.0, hypothesis-5.35.1, xdist-3.3.1, subtests-0.13.1, rerunfailures-14.0, flakefinder-1.1.0, cpp-2.3.0, anyio-4.10.0, asyncio-1.1.0
asyncio: mode=Mode.STRICT, asyncio_default_fixture_loop_scope=None, asyncio_default_test_loop_scope=function
collecting ... collected 4 items / 3 deselected / 1 selected
Running 1 items in this shard

test/distributed/tensor/test_dtensor_export.py::DTensorExportTest::test_codegen joint_gm
class inner_f(torch.nn.Module):
    def forward(self, primals_1: "f32[4, 10]", primals_2: "f32[4]", primals_3: "f32[10, 4]", primals_4: "f32[10]", primals_5: "f32[4, 10]", primals_6: "f32[4]", primals_7: "f32[10, 4]", primals_8: "f32[10]", primals_9: "f32[20, 10]", tangents_1: "f32[20, 10]"):
        # No stacktrace found for following nodes
        t: "f32[10, 4]" = torch.ops.aten.t.default(primals_1);  primals_1 = None
        addmm: "f32[20, 4]" = torch.ops.aten.addmm.default(primals_2, primals_9, t);  primals_2 = t = None
        view: "f32[20, 4]" = torch.ops.aten.view.default(addmm, [20, 4]);  addmm = None
        relu: "f32[20, 4]" = torch.ops.aten.relu.default(view);  view = None
        detach: "f32[20, 4]" = torch.ops.aten.detach.default(relu)
        detach_1: "f32[20, 4]" = torch.ops.aten.detach.default(detach);  detach = None
        view_1: "f32[20, 4]" = torch.ops.aten.view.default(relu, [20, 4]);  relu = None
        t_1: "f32[4, 10]" = torch.ops.aten.t.default(primals_3);  primals_3 = None
        div: "f32[10]" = torch.ops.aten.div.Tensor(primals_4, 4);  primals_4 = None
        addmm_1: "f32[20, 10]" = torch.ops.aten.addmm.default(div, view_1, t_1);  div = None
        all_reduce: "f32[20, 10]" = torch.ops._c10d_functional.all_reduce.default(addmm_1, 'sum', '5');  addmm_1 = None
        wait_tensor: "f32[20, 10]" = torch.ops._c10d_functional.wait_tensor.default(all_reduce);  all_reduce = None
        view_2: "f32[20, 10]" = torch.ops.aten.view.default(wait_tensor, [20, 10]);  wait_tensor = None
        view_3: "f32[20, 10]" = torch.ops.aten.view.default(view_2, [20, 10]);  view_2 = None
        t_2: "f32[10, 4]" = torch.ops.aten.t.default(primals_5);  primals_5 = None
        addmm_2: "f32[20, 4]" = torch.ops.aten.addmm.default(primals_6, view_3, t_2);  primals_6 = None
        view_4: "f32[20, 4]" = torch.ops.aten.view.default(addmm_2, [20, 4]);  addmm_2 = None
        relu_1: "f32[20, 4]" = torch.ops.aten.relu.default(view_4);  view_4 = None
        detach_2: "f32[20, 4]" = torch.ops.aten.detach.default(relu_1)
        detach_3: "f32[20, 4]" = torch.ops.aten.detach.default(detach_2);  detach_2 = None
        view_5: "f32[20, 4]" = torch.ops.aten.view.default(relu_1, [20, 4]);  relu_1 = None
        t_3: "f32[4, 10]" = torch.ops.aten.t.default(primals_7);  primals_7 = None
        div_1: "f32[10]" = torch.ops.aten.div.Tensor(primals_8, 4);  primals_8 = None
        addmm_3: "f32[20, 10]" = torch.ops.aten.addmm.default(div_1, view_5, t_3);  div_1 = None
        all_reduce_1: "f32[20, 10]" = torch.ops._c10d_functional.all_reduce.default(addmm_3, 'sum', '5');  addmm_3 = None
        wait_tensor_1: "f32[20, 10]" = torch.ops._c10d_functional.wait_tensor.default(all_reduce_1);  all_reduce_1 = None
        view_6: "f32[20, 10]" = torch.ops.aten.view.default(wait_tensor_1, [20, 10]);  wait_tensor_1 = None
        t_4: "f32[10, 4]" = torch.ops.aten.t.default(t_3);  t_3 = None
        mm: "f32[20, 4]" = torch.ops.aten.mm.default(tangents_1, t_4);  t_4 = None
        t_5: "f32[10, 20]" = torch.ops.aten.t.default(tangents_1)
        mm_1: "f32[10, 4]" = torch.ops.aten.mm.default(t_5, view_5);  t_5 = view_5 = None
        t_6: "f32[4, 10]" = torch.ops.aten.t.default(mm_1);  mm_1 = None
        sum_1: "f32[1, 10]" = torch.ops.aten.sum.dim_IntList(tangents_1, [0], True);  tangents_1 = None
        view_7: "f32[10]" = torch.ops.aten.view.default(sum_1, [10]);  sum_1 = None
        t_7: "f32[10, 4]" = torch.ops.aten.t.default(t_6);  t_6 = None
        detach_4: "f32[20, 4]" = torch.ops.aten.detach.default(detach_3);  detach_3 = None
        detach_5: "f32[20, 4]" = torch.ops.aten.detach.default(detach_4);  detach_4 = None
        threshold_backward: "f32[20, 4]" = torch.ops.aten.threshold_backward.default(mm, detach_5, 0);  mm = detach_5 = None
        t_8: "f32[4, 10]" = torch.ops.aten.t.default(t_2);  t_2 = None
        mm_2: "f32[20, 10]" = torch.ops.aten.mm.default(threshold_backward, t_8);  t_8 = None
        t_9: "f32[4, 20]" = torch.ops.aten.t.default(threshold_backward)
        mm_3: "f32[4, 10]" = torch.ops.aten.mm.default(t_9, view_3);  t_9 = view_3 = None
        t_10: "f32[10, 4]" = torch.ops.aten.t.default(mm_3);  mm_3 = None
        sum_2: "f32[1, 4]" = torch.ops.aten.sum.dim_IntList(threshold_backward, [0], True);  threshold_backward = None
        view_8: "f32[4]" = torch.ops.aten.view.default(sum_2, [4]);  sum_2 = None
        t_11: "f32[4, 10]" = torch.ops.aten.t.default(t_10);  t_10 = None
        all_reduce_2: "f32[20, 10]" = torch.ops._c10d_functional.all_reduce.default(mm_2, 'sum', '5');  mm_2 = None
        wait_tensor_2: "f32[20, 10]" = torch.ops._c10d_functional.wait_tensor.default(all_reduce_2);  all_reduce_2 = None
        t_12: "f32[10, 4]" = torch.ops.aten.t.default(t_1);  t_1 = None
        mm_4: "f32[20, 4]" = torch.ops.aten.mm.default(wait_tensor_2, t_12);  t_12 = None
        t_13: "f32[10, 20]" = torch.ops.aten.t.default(wait_tensor_2)
        mm_5: "f32[10, 4]" = torch.ops.aten.mm.default(t_13, view_1);  t_13 = view_1 = None
        t_14: "f32[4, 10]" = torch.ops.aten.t.default(mm_5);  mm_5 = None
        sum_3: "f32[1, 10]" = torch.ops.aten.sum.dim_IntList(wait_tensor_2, [0], True);  wait_tensor_2 = None
        view_9: "f32[10]" = torch.ops.aten.view.default(sum_3, [10]);  sum_3 = None
        t_15: "f32[10, 4]" = torch.ops.aten.t.default(t_14);  t_14 = None
        detach_6: "f32[20, 4]" = torch.ops.aten.detach.default(detach_1);  detach_1 = None
        detach_7: "f32[20, 4]" = torch.ops.aten.detach.default(detach_6);  detach_6 = None
        threshold_backward_1: "f32[20, 4]" = torch.ops.aten.threshold_backward.default(mm_4, detach_7, 0);  mm_4 = detach_7 = None
        t_16: "f32[4, 20]" = torch.ops.aten.t.default(threshold_backward_1)
        mm_6: "f32[4, 10]" = torch.ops.aten.mm.default(t_16, primals_9);  t_16 = primals_9 = None
        t_17: "f32[10, 4]" = torch.ops.aten.t.default(mm_6);  mm_6 = None
        sum_4: "f32[1, 4]" = torch.ops.aten.sum.dim_IntList(threshold_backward_1, [0], True);  threshold_backward_1 = None
        view_10: "f32[4]" = torch.ops.aten.view.default(sum_4, [4]);  sum_4 = None
        t_18: "f32[4, 10]" = torch.ops.aten.t.default(t_17);  t_17 = None
        return [view_6, t_18, view_10, t_15, view_9, t_11, view_8, t_7, view_7, None]
        
fw_gm
class GraphModule(torch.nn.Module):
    def forward(self, primals_1: "f32[4, 10]", primals_2: "f32[4]", primals_3: "f32[10, 4]", primals_4: "f32[10]", primals_5: "f32[4, 10]", primals_6: "f32[4]", primals_7: "f32[10, 4]", primals_8: "f32[10]", primals_9: "f32[20, 10]"):
        # No stacktrace found for following nodes
        t: "f32[10, 4]" = torch.ops.aten.t.default(primals_1);  primals_1 = None
        addmm: "f32[20, 4]" = torch.ops.aten.addmm.default(primals_2, primals_9, t);  primals_2 = t = None
        view: "f32[20, 4]" = torch.ops.aten.view.default(addmm, [20, 4]);  addmm = None
        relu: "f32[20, 4]" = torch.ops.aten.relu.default(view);  view = None
        view_1: "f32[20, 4]" = torch.ops.aten.view.default(relu, [20, 4])
        t_1: "f32[4, 10]" = torch.ops.aten.t.default(primals_3);  primals_3 = None
        div: "f32[10]" = torch.ops.aten.div.Tensor(primals_4, 4);  primals_4 = None
        addmm_1: "f32[20, 10]" = torch.ops.aten.addmm.default(div, view_1, t_1);  div = view_1 = None
        all_reduce: "f32[20, 10]" = torch.ops._c10d_functional.all_reduce.default(addmm_1, 'sum', '5');  addmm_1 = None
        wait_tensor: "f32[20, 10]" = torch.ops._c10d_functional.wait_tensor.default(all_reduce);  all_reduce = None
        view_2: "f32[20, 10]" = torch.ops.aten.view.default(wait_tensor, [20, 10]);  wait_tensor = None
        view_3: "f32[20, 10]" = torch.ops.aten.view.default(view_2, [20, 10]);  view_2 = None
        t_2: "f32[10, 4]" = torch.ops.aten.t.default(primals_5);  primals_5 = None
        addmm_2: "f32[20, 4]" = torch.ops.aten.addmm.default(primals_6, view_3, t_2);  primals_6 = None
        view_4: "f32[20, 4]" = torch.ops.aten.view.default(addmm_2, [20, 4]);  addmm_2 = None
        relu_1: "f32[20, 4]" = torch.ops.aten.relu.default(view_4);  view_4 = None
        view_5: "f32[20, 4]" = torch.ops.aten.view.default(relu_1, [20, 4])
        t_3: "f32[4, 10]" = torch.ops.aten.t.default(primals_7);  primals_7 = None
        div_1: "f32[10]" = torch.ops.aten.div.Tensor(primals_8, 4);  primals_8 = None
        addmm_3: "f32[20, 10]" = torch.ops.aten.addmm.default(div_1, view_5, t_3);  div_1 = view_5 = None
        all_reduce_1: "f32[20, 10]" = torch.ops._c10d_functional.all_reduce.default(addmm_3, 'sum', '5');  addmm_3 = None
        wait_tensor_1: "f32[20, 10]" = torch.ops._c10d_functional.wait_tensor.default(all_reduce_1);  all_reduce_1 = None
        view_6: "f32[20, 10]" = torch.ops.aten.view.default(wait_tensor_1, [20, 10]);  wait_tensor_1 = None
        t_4: "f32[10, 4]" = torch.ops.aten.t.default(t_3);  t_3 = None
        t_8: "f32[4, 10]" = torch.ops.aten.t.default(t_2);  t_2 = None
        t_12: "f32[10, 4]" = torch.ops.aten.t.default(t_1);  t_1 = None
        return (view_6, primals_9, relu, view_3, relu_1, t_4, t_8, t_12)
        
bw_gm
class GraphModule(torch.nn.Module):
    def forward(self, primals_9: "f32[20, 10]", relu: "f32[20, 4]", view_3: "f32[20, 10]", relu_1: "f32[20, 4]", t_4: "f32[10, 4]", t_8: "f32[4, 10]", t_12: "f32[10, 4]", tangents_1: "f32[20, 10]"):
        # No stacktrace found for following nodes
        mm: "f32[20, 4]" = torch.ops.aten.mm.default(tangents_1, t_4);  t_4 = None
        t_5: "f32[10, 20]" = torch.ops.aten.t.default(tangents_1)
        view_5: "f32[20, 4]" = torch.ops.aten.view.default(relu_1, [20, 4])
        mm_1: "f32[10, 4]" = torch.ops.aten.mm.default(t_5, view_5);  t_5 = view_5 = None
        t_6: "f32[4, 10]" = torch.ops.aten.t.default(mm_1);  mm_1 = None
        sum_1: "f32[1, 10]" = torch.ops.aten.sum.dim_IntList(tangents_1, [0], True);  tangents_1 = None
        view_7: "f32[10]" = torch.ops.aten.view.default(sum_1, [10]);  sum_1 = None
        t_7: "f32[10, 4]" = torch.ops.aten.t.default(t_6);  t_6 = None
        detach_2: "f32[20, 4]" = torch.ops.aten.detach.default(relu_1);  relu_1 = None
        detach_3: "f32[20, 4]" = torch.ops.aten.detach.default(detach_2);  detach_2 = None
        detach_4: "f32[20, 4]" = torch.ops.aten.detach.default(detach_3);  detach_3 = None
        detach_5: "f32[20, 4]" = torch.ops.aten.detach.default(detach_4);  detach_4 = None
        threshold_backward: "f32[20, 4]" = torch.ops.aten.threshold_backward.default(mm, detach_5, 0);  mm = detach_5 = None
        mm_2: "f32[20, 10]" = torch.ops.aten.mm.default(threshold_backward, t_8);  t_8 = None
        t_9: "f32[4, 20]" = torch.ops.aten.t.default(threshold_backward)
        mm_3: "f32[4, 10]" = torch.ops.aten.mm.default(t_9, view_3);  t_9 = view_3 = None
        t_10: "f32[10, 4]" = torch.ops.aten.t.default(mm_3);  mm_3 = None
        sum_2: "f32[1, 4]" = torch.ops.aten.sum.dim_IntList(threshold_backward, [0], True);  threshold_backward = None
        view_8: "f32[4]" = torch.ops.aten.view.default(sum_2, [4]);  sum_2 = None
        t_11: "f32[4, 10]" = torch.ops.aten.t.default(t_10);  t_10 = None
        all_reduce_2: "f32[20, 10]" = torch.ops._c10d_functional.all_reduce.default(mm_2, 'sum', '5');  mm_2 = None
        wait_tensor_2: "f32[20, 10]" = torch.ops._c10d_functional.wait_tensor.default(all_reduce_2);  all_reduce_2 = None
        mm_4: "f32[20, 4]" = torch.ops.aten.mm.default(wait_tensor_2, t_12);  t_12 = None
        t_13: "f32[10, 20]" = torch.ops.aten.t.default(wait_tensor_2)
        view_1: "f32[20, 4]" = torch.ops.aten.view.default(relu, [20, 4])
        mm_5: "f32[10, 4]" = torch.ops.aten.mm.default(t_13, view_1);  t_13 = view_1 = None
        t_14: "f32[4, 10]" = torch.ops.aten.t.default(mm_5);  mm_5 = None
        sum_3: "f32[1, 10]" = torch.ops.aten.sum.dim_IntList(wait_tensor_2, [0], True);  wait_tensor_2 = None
        view_9: "f32[10]" = torch.ops.aten.view.default(sum_3, [10]);  sum_3 = None
        t_15: "f32[10, 4]" = torch.ops.aten.t.default(t_14);  t_14 = None
        detach: "f32[20, 4]" = torch.ops.aten.detach.default(relu);  relu = None
        detach_1: "f32[20, 4]" = torch.ops.aten.detach.default(detach);  detach = None
        detach_6: "f32[20, 4]" = torch.ops.aten.detach.default(detach_1);  detach_1 = None
        detach_7: "f32[20, 4]" = torch.ops.aten.detach.default(detach_6);  detach_6 = None
        threshold_backward_1: "f32[20, 4]" = torch.ops.aten.threshold_backward.default(mm_4, detach_7, 0);  mm_4 = detach_7 = None
        t_16: "f32[4, 20]" = torch.ops.aten.t.default(threshold_backward_1)
        mm_6: "f32[4, 10]" = torch.ops.aten.mm.default(t_16, primals_9);  t_16 = primals_9 = None
        t_17: "f32[10, 4]" = torch.ops.aten.t.default(mm_6);  mm_6 = None
        sum_4: "f32[1, 4]" = torch.ops.aten.sum.dim_IntList(threshold_backward_1, [0], True);  threshold_backward_1 = None
        view_10: "f32[4]" = torch.ops.aten.view.default(sum_4, [4]);  sum_4 = None
        t_18: "f32[4, 10]" = torch.ops.aten.t.default(t_17);  t_17 = None
        return (t_18, view_10, t_15, view_9, t_11, view_8, t_7, view_7, None)
        

>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>> PDB set_trace >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>
> /data/users/bahuang/pytorch/test/distributed/tensor/test_dtensor_export.py(186)test_codegen()
-> joint_with_descriptors.params_spec
(Pdb) --KeyboardInterrupt--
(Pdb) 